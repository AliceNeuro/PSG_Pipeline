{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a37b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8d7a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SpO2 artifact', 'Arousal ()', 'Hypopnea', 'SpO2 desaturation', 'Obstructive Apnea', 'Central Apnea', 'Arousal (STANDARD)', 'Mixed Apnea', 'Arousal (ASDA)', 'Unsure', 'Arousal (CHESHIRE)', 'Arousal (Standard)']\n"
     ]
    }
   ],
   "source": [
    "import xmltodict\n",
    "import os\n",
    "\n",
    "full_names = []\n",
    "annot_folder = \"/wynton/group/andrews/data/shhs/nsrr/polysomnography/annotations-events-profusion/shhs1/\"\n",
    "for file in os.listdir(annot_folder)[:300]:\n",
    "    if file.endswith(\".xml\"):\n",
    "        annot_path = os.path.join(annot_folder, file)\n",
    "        with open(annot_path, encoding='utf-8') as f:\n",
    "            info_dict = xmltodict.parse(f.read())\n",
    "        events = info_dict['CMPStudyConfig']['ScoredEvents']['ScoredEvent']\n",
    "        for event in events:\n",
    "            name = event['Name']\n",
    "            if name not in full_names:\n",
    "                full_names.append(name)\n",
    "print(full_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2350adeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = \"/wynton/group/andrews/data/PSG_Pipeline_Outputs/extracted_features/hsp_mgb/run_20251218/hsp_mgb_all.csv\"\n",
    "df = pd.read_csv(path)\n",
    "print(\"DOMINANT\")\n",
    "df[[\"AHI_NREM\",\"AHI_REM\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ad739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = \"/wynton/group/andrews/data/PSG_Pipeline_Outputs/extracted_features/hsp_mgb/run_20251218/hsp_mgb_all.csv\"\n",
    "df = pd.read_csv(path)\n",
    "print(\"ONSET\")\n",
    "df[[\"AHI_NREM\",\"AHI_REM\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = \"/wynton/group/andrews/data/PSG_Pipeline_Outputs/extracted_features/hsp_mgb/run_20251218/hsp_mgb_all.csv\"\n",
    "df = pd.read_csv(path)\n",
    "print(\"OFFSET\")\n",
    "df[[\"AHI_NREM\",\"AHI_REM\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf79ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/wynton/group/andrews/data/HSP/PSG/bids/MGB\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "for sub in os.listdir(path)[:200]:\n",
    "    sub_path = os.path.join(path,sub)\n",
    "    for ses in os.listdir(sub_path):\n",
    "        ses_path = os.path.join(sub_path, ses, \"eeg\")\n",
    "        if os.path.exists(ses_path):\n",
    "            for file in os.listdir(ses_path):\n",
    "                if \"channels\" in file:\n",
    "                    df = pd.read_csv(os.path.join(ses_path, file), sep = \"\\t\")\n",
    "                    names = df[\"name\"].to_list()\n",
    "                    if \"NPT\" in names:\n",
    "                        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5f13e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "psg_ids = [\n",
    "    (\"S0001114191434\", 1),\n",
    "    (\"S0001116639282\", 1),\n",
    "    (\"S0001121913093\", 1),\n",
    "    (\"S0001122312754\", 1),\n",
    "]\n",
    "\n",
    "base_path = \"/wynton/group/andrews/data/HSP/PSG/bids/MGB/\"\n",
    "\n",
    "# Set pandas display options to show all rows/columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Loop over subjects\n",
    "for sub_id, session in psg_ids:\n",
    "    eeg_path = os.path.join(base_path, f\"sub-{sub_id}\", f\"ses-{session}\", \"eeg\")\n",
    "    \n",
    "    if not os.path.exists(eeg_path):\n",
    "        print(f\"[WARNING] Path not found: {eeg_path}\")\n",
    "        continue\n",
    "\n",
    "    # Look for TSV files (usually *_channels.tsv in BIDS EEG)\n",
    "    tsv_files = [f for f in os.listdir(eeg_path) if f.endswith(\"channels.tsv\")]\n",
    "    if not tsv_files:\n",
    "        print(f\"[WARNING] No channels TSV found in {eeg_path}\")\n",
    "        continue\n",
    "\n",
    "    # Open the first TSV found\n",
    "    tsv_file_path = os.path.join(eeg_path, tsv_files[0])\n",
    "    df_channels = pd.read_csv(tsv_file_path, sep='\\t')\n",
    "    \n",
    "    print(f\"\\nSubject {sub_id}, session {session}: {tsv_file_path}\")\n",
    "    print(df_channels['name'])  # Print the full dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4441d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path_log = \"/wynton/home/leng/alice-albrecht/projects/PSG_Pipeline/log/vb_shhs1.o1611326\"\n",
    "path_features = \"/wynton/group/andrews/data/PSG_Pipeline_Outputs/extracted_features/shhs_ses-1/run_20251216/\"\n",
    "\n",
    "selected_subjects = []\n",
    "\n",
    "with open(path_log, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines: \n",
    "        if \"[ERROR] MATLAB\" in line:\n",
    "            psg_id = line.split('.mat')[0].split('sub-')[-1]\n",
    "            sub_id = psg_id.split('_')[0]\n",
    "            session = psg_id.split('ses-')[-1]\n",
    "            selected_subjects.append((sub_id, int(session)))\n",
    "            # delete files\n",
    "            file = os.path.join(path_features,\n",
    "                    f\"shhs_ses-1_sub-{sub_id}_extracted_features.csv\")\n",
    "            if os.path.exists(file):\n",
    "                #print(file)\n",
    "                os.remove(file)\n",
    "            # delete file wide\n",
    "            file_wide = os.path.join(path_features,\n",
    "                    f\"shhs_ses-1_sub-{sub_id}_extracted_features_wide.csv\")\n",
    "            if os.path.exists(file_wide):\n",
    "                #print(file_wide)\n",
    "                os.remove(file_wide)\n",
    "# Remove duplicates\n",
    "selected_subjects = list(set(selected_subjects))\n",
    "\n",
    "print(selected_subjects)\n",
    "print(len(selected_subjects), \"subjects/sessions with missing features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78705f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of files to keep \n",
    "master = pd.read_csv(\"/wynton/group/andrews/data/PSG_Pipeline_Outputs/mastersheets/shhs_ses-1_mastersheet.csv\")\n",
    "\n",
    "\n",
    "for sub in \n",
    "master[\"h5_path\"] = master[\"h5_path\"].str.split('/').str[-1]\n",
    "keep_files = set(master[\"h5_path\"].to_list())\n",
    "\n",
    "# Go though all the files computed\n",
    "path_h5 = \"/wynton/group/andrews/data/PSG_Pipeline_Outputs/h5_data/hsp_mgb/\"\n",
    "for file in os.listdir(path_h5):\n",
    "    if file not in keep_files:\n",
    "        print(f\"Deleting: {file}\")\n",
    "        #os.remove(os.path.join(path_h5, file)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a85eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_log = \"/wynton/home/leng/alice-albrecht/projects/PSG_Pipeline/log/h5_shhs1.o1467406\"\n",
    "# path_log = \"/wynton/home/leng/alice-albrecht/projects/PSG_Pipeline/log/burdens_shhs2.o1467690\"\n",
    "selected_subjects = []\n",
    "\n",
    "with open(path_log, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for i, line in enumerate(lines[:-1]):  # avoid going past last line\n",
    "    if \"No RESP\" in line and \"No RESP\" in lines[i+1]:\n",
    "        psg_id = line.split('sub-')[-1].strip()\n",
    "        sub_id = psg_id.split('_')[0]\n",
    "        session = psg_id.split('ses-')[-1]\n",
    "        selected_subjects.append((sub_id, int(session)))\n",
    "        print(line)\n",
    "        print(lines[i+1])\n",
    "\n",
    "# Remove duplicates\n",
    "selected_subjects = list(set(selected_subjects))\n",
    "\n",
    "print(selected_subjects)\n",
    "print(len(selected_subjects), \"subjects/sessions with missing features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff45237",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_log = \"/wynton/home/leng/alice-albrecht/projects/PSG_Pipeline/log/burdens_shhs1.o1467689\"\n",
    "# path_log = \"/wynton/home/leng/alice-albrecht/projects/PSG_Pipeline/log/burdens_shhs2.o1467690\"\n",
    "selected_subjects = []\n",
    "with open(path_log, \"r\") as f:\n",
    "    for line in f:\n",
    "        if \"subprocess.CalledProcessError:\" in line:\n",
    "            psg_id = line.split('.mat')[0].split('sub-')[-1]\n",
    "            sub_id = psg_id.split('_')[0]\n",
    "            session = psg_id.split('ses-')[-1]\n",
    "            selected_subjects.append((sub_id, int(session)))\n",
    "        if \"Feature 'vb' was not computed\" in line:\n",
    "            sub_id = line.split(':')[0].split('Sub ')[-1].strip()\n",
    "            session = 1\n",
    "            selected_subjects.append((sub_id, int(session)))\n",
    "\n",
    "print(len(selected_subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc7582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "from collections import Counter\n",
    "\n",
    "annot_path = \"/wynton/group/andrews/data/MrOS/mros-sof_mjhe/vs/EDF/bi/bi0002.edf.XML\"\n",
    "with open(annot_path, encoding='utf-8') as f:\n",
    "        info_dict = xmltodict.parse(f.read())\n",
    "\n",
    "events = info_dict['CMPStudyConfig']['ScoredEvents']['ScoredEvent']\n",
    "sleep_stages = info_dict['CMPStudyConfig']['SleepStages']['SleepStage']\n",
    "names = [event['Name'] for event in events if 'Name' in event]\n",
    "Counter(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbd7b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "df = pd.read_csv(\"/Users/alicealbrecht/Desktop/S0001_psg_metadata_2025-09-05.csv\")\n",
    "print(df.shape)\n",
    "df = df.rename(columns={\n",
    "    \"BDSPPatientID\": \"sub_id\",\n",
    "    \"SessionID\": \"session\"\n",
    "})\n",
    "df['sub_id'] = 'S0001' + df['sub_id'].astype(str)\n",
    "df['session'] = df['session'].astype(int)\n",
    "df = df[\n",
    "    df['StudyType'].str.lower().str.contains(\"diagnostic|dignostic\", na=False) &\n",
    "    (~df['StudyType'].str.lower().str.contains(\"oxygen\", na=False))\n",
    "]\n",
    "print(df.shape)\n",
    "df['StudyType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeef58d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "master = pd.read_csv(\"/Users/alicealbrecht/wynton_data/PSG_Pipeline_Outputs/mastersheets/hsp_mgb_mastersheet_before_diagnostic.csv\")\n",
    "print(master.shape)\n",
    "master['sub_id'] = master['sub_id'].astype(str)\n",
    "master['session'] = master['session'].astype(int)\n",
    "master = master.merge(\n",
    "    df[['sub_id', 'session', 'StudyType']],\n",
    "    on=['sub_id', 'session'],\n",
    "    how='left'\n",
    ")\n",
    "master = master[\n",
    "    master['StudyType'].str.lower().str.contains(\"diagnostic|dignostic\", na=False) &\n",
    "    (~master['StudyType'].str.lower().str.contains(\"oxygen\", na=False))\n",
    "]\n",
    "print(master.shape)\n",
    "master['StudyType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b689c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "master = master.drop(columns= ['StudyType'])\n",
    "master.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cf1909",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/Users/alicealbrecht/wynton_data/PSG_Pipeline_Outputs/mastersheets/hsp_mgb_mastersheet.csv\"\n",
    "master.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074264c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "master = master[~master['annot_path'].isna()]\n",
    "print(master.shape)\n",
    "# Look at all possible annotaiotn file found\n",
    "possible_annot = master['annot_path'].str.extract(r'ses-\\d+_(.*)')[0]\n",
    "possible_annot.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c195d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "master = pd.read_csv(\"/Users/alicealbrecht/wynton_data/PSG_Pipeline_Outputs/mastersheets/hsp_mgb_mastersheet.csv\")\n",
    "master['annot_options'] = master['annot_path'].str.extract(r'ses-\\d+_(.*)')[0]\n",
    "master['annot_options'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a862b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "master.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4a9131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_sleep_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
